<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Ripple HVX Optimization Guide</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-87587ea3.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-8f2f9eff.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">Ripple HVX Optimization Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="abstract"><a class="header" href="#abstract">Abstract</a></h1>
<p>This guide presents general optimization tips when writing
Ripple code to Hexagon HVX SIMD hardware.
We start with general optimization advice about writing Ripple code for SIMD hardware, which we complete with specific advice about writing to HVX.
Then, we present profiling API and discuss its usage.
Finally, we give instructions to install LLDB in the VS Code IDE.</p>
<hr>
<p><em>Copyright (c) 2024-2025 Qualcomm Innovation Center, Inc. All rights reserved.
SPDX-License-Identifier: BSD-3-Clause-Clear</em></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="license"><a href="#license" class="header">License</a></h1>
<p>Clear 3-clause BSD License</p>
<p>Copyright (c) 2025 Qualcomm Technologies, Inc. All rights reserved.</p>
<p>Redistribution and use in source and binary forms, with or without modification, are permitted (subject to the limitations in the disclaimer below) provided that the following conditions are met:</p>
<ul>
<li>
<p>Redistributions of source code must retain the above copyright notice,
this list of conditions and the following disclaimer.</p>
</li>
<li>
<p>Redistributions in binary form must reproduce the above copyright
notice, this list of conditions and the following disclaimer in the
documentation and/or other materials provided with the distribution.</p>
</li>
<li>
<p>Neither the name of the copyright holder nor the names of its
contributors may be used to endorse or promote products derived from this
software without specific prior written permission.
NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY’S PATENT RIGHTS ARE GRANTED BY THIS LICENSE. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="machine-independent-ripple-optimization-and-troubleshooting"><a class="header" href="#machine-independent-ripple-optimization-and-troubleshooting">Machine-independent Ripple optimization and troubleshooting</a></h1>
<h1 id="be-mindful-of-type-conversions"><a class="header" href="#be-mindful-of-type-conversions">Be mindful of type conversions</a></h1>
<h2 id="converting-ripple-indexes"><a class="header" href="#converting-ripple-indexes">Converting Ripple indexes</a></h2>
<p><strong>Performance impact</strong>: High.</p>
<p>We highly recommend that you do not convert expressions coming from <code>ripple_id()</code> indices, if they are used in memory references.</p>
<p>For instance, on a 64-bit machine, the following code narrows the Ripple index before using it in a array reference:</p>
<pre><code class="language-C">1:  void vadd(float * a, float *b, float *c) {
2:    ripple_block_t b = ripple_set_block_shape(VECTOR_PE, 32);
3:    size_t v = ripple_id(b, 0);
4:    unsigned v_shifted = v + 2;
5:    c[rid] = a[v] + b[v_shifted];
6:  }
</code></pre>
<p>The cast from <code>size_t</code> (which is 64-bit on a 64-bit machine) to <code>unsigned</code> (which we assume to be 32-bit on that same machine) on line 4 is an issue, because clang needs into account the fact that using a <code>unsigned</code> forces the maximum value for v_shifted to unsigned’s maximum value, <code>2^32 -1</code>.
Numerically, the cast is similar to computing <code>a[(v + 2) % (2^32-1)]</code>.
The <code>%</code> modulo can make it challenging for Ripple to analyze the memory access stride between vector lanes in the <code>a[rid % 65535]</code> access.
Hence, there is a risk that Ripple will miss that this is a coalesced access,
in which case a sequential (non-vectorized) memory access
will be generated.
Note that coalescing analysis typically improves as Ripple improves.
The safe option here is to avoid casting expression that depend upon a ripple_id and that determine a memory reference to be casted to a narrower type.</p>
<h1 id="general-vector-optimization-principles"><a class="header" href="#general-vector-optimization-principles">General vector optimization principles</a></h1>
<p>In this section, we discuss general optimization principles when targeting a
machine that has a vector engine.</p>
<p>Consider optimization as trying to avoid doing expensive things:</p>
<ul>
<li><em>Utilization</em> is about putting all the vector lanes available to contribution.</li>
<li><em>Coalescing</em> is about performing loads and stores efficiently,
in as few chunks of contiguous data as possible.</li>
<li><em>Alignment</em> is about loading and storing a data chunk
that exactly matches one of the “hardware chunks” that the memory is made of.</li>
<li><em>Register reuse</em> is about avoiding unnecessary loads and stores
by doing all the work needed on a given vector
while the data is in the (vector) registers.</li>
</ul>
<h2 id="utilization"><a class="header" href="#utilization">Utilization</a></h2>
<p>Utilization is defined by how many vector lanes are active in the computation.
It is recommended to grow the number of vector lanes to fully utilize at least
one HVX vector.</p>
<h3 id="underutilization"><a class="header" href="#underutilization">Underutilization</a></h3>
<p><strong>Performance impact</strong>: Medium.</p>
<p>The number of hardware vector lanes utilized by our code depends upon
the type of the tensor elements we are manipulating.
Let <code>elem_size</code> be the number of bits in our tensor elements,
and <code>vec_bits</code> the number of bits in our vector engine.
For example, for HVX, <code>vec_bits = 1024</code>.
It is given by <code>#v = ceiling((block_size x elem_size)  / vec_bits)</code>.</p>
<p>For AVX512, <code>vec_bits = 512</code>, resulting in half the number of vector lanes for any specific data type.</p>
<p>This usually means that a good Ripple block size is one that is at least as big
as the number of vector lanes offered by the underlying hardware vector engine.
Using more can be beneficial, because it exposes more parallel computations,
which can be used by the compiler to get better performance
(typically by improving instruction-level parallelism,
like pipeline or VLIW parallelism).</p>
<h3 id="overutilization"><a class="header" href="#overutilization">Overutilization</a></h3>
<p><strong>Performance impact</strong>: High.</p>
<p>If the number of active vector registers in a Ripple block of vector lanes
goes beyond the size of the vector register file
(e.g. HVX defines 32 vector registers per thread),
the compiler may have to save
registers to a slower memory and load them back later,
which significantly lowers performance.</p>
<p>Beware that compilers often introduce temporary computations,
which also take up register space and may lead to those extraneous store/loads.</p>
<h2 id="coalescing"><a class="header" href="#coalescing">Coalescing</a></h2>
<p><strong>Performance impact</strong>: High.</p>
<p>In vector hardware, loads and stores happen by chunks of contiguous memory
(typically of the native vector size).
Because they are slower than other instructions,
the amount of loads and stores in a program
tend to have a significant impact on performance.
Hence, reducing the number of loads and stores is an important step in the optimization of SIMD programs.
This impacts how we should load and store the data we manipulate.</p>
<p>When we only load and store contiguous data chunks that correspond to
full native SIMD vectors, the number of loads and stores is minimized.
Such loads and stores are called <em>coalesced</em>.
While we depict the general goal here, this manual contains a section about specific ways to <a href="coalescing">obtain coalescing</a></p>
<p>We want to load contiguous memory elements to contiguous processing elements
(i.e. vector lanes).
If we are using a 1-dimensional block shape, this means that
only the rightmost dimension of a tensor’s access function must depend upon
the vector lane id, and that in that access function,
the vector lane id is not multiplied by anything (i.e. its coefficient is 1).</p>
<p>For example:</p>
<pre><code class="language-C">v = ripple_id(ripple_set_block_shape(0, 32), 0);
x = A[i][k][j + v];
</code></pre>
<p>If <code>v</code> is multiplied by a constant (other than 1),
this constant becomes a <em>stride</em>,
meaning that some elements of the tensor are skipped, which is less efficient
than a coalesced load.</p>
<p>If <code>v</code> is involved in other dimensions than the rightmost in a tensor access,
this results in a stride as well
(as large as the slice defined by the dimensions to the right of that
involving <code>v</code>).
So if the semantics of our computations allow for it, we only use <code>v</code> in the
rightmost dimension of our tensor references.
If we don’t, the code will be correct but slow.</p>
<p>Ripple lays out block elements into vector lanes along the first block dimension, then the second, etc.</p>
<h2 id="alignment"><a class="header" href="#alignment">Alignment</a></h2>
<p><strong>Performance impact</strong> : Medium.</p>
<p>Hardware memory space is typically partitioned regularly as
a large set of fixed-size chunks.
An example of this is cache lines.
Because of this partitioning,
the most efficient memory transfer (load or store) that can be done is by
transferring exactly a set of full chunks.
We have seen above that a condition for this to happen is for such
load or store to be coalesced.
For a vector engine to load exactly one chunk,
the start address of a coalesced load or store also has to start
at the starting address of a chunk.
We call such a load or store “aligned”.
The basic rule to have an aligned load or store is for its start address
(the address accessed by coordinate <code>(0)</code> or <code>(0, 0)</code>) to be a multiple of the
hardware vector size (e.g. 1024 bits, i.e. 128 bytes, for HVX).</p>
<h3 id="specifying-alignment"><a class="header" href="#specifying-alignment">Specifying Alignment</a></h3>
<p>For best result, we advise the use of the ripple API to specify tensor
alignment, as well as using them as close to the load/store instruction as
possible. We provide
<code>ripple_ptr_alignment</code> and <code>ripple_ptr_alignment_slice</code> functions to specify the
pointer alignment from a tensor of pointers.</p>
<ul>
<li>
<p><code>ripple_ptr_alignment_slice(Tensor_of_Pointers, Alignment_in_Bytes)</code></p>
<p>This construct indicates that the element <code>(0, 0 ... 0, 0)</code> is aligned by the provided alignment.</p>
</li>
<li>
<p><code>ripple_ptr_alignment_slice(Tensor_of_Pointers, Alignment_in_Bytes, Slice_Index0, ...)</code></p>
<p>This constructs is similar to <code>ripple_ptr_alignment_slice</code>, with the addition of letting you specify the slice indices to extract the pointer which is aligned. Non-provided indices is assumed to be zero.</p>
</li>
</ul>
<h4 id="considerations"><a class="header" href="#considerations">Considerations</a></h4>
<p>By using this API, you are specifying alignment constraints that will be
followed by the compiler. If, at runtime, the pointer is not aligned to
the provided value, the behavior will be hardware defined (e.g., an hardware interrupt may be raised, or
the values may be loaded/stored by ignoring the pointer bits that are not aligned).</p>
<h3 id="using-the-ripple-vector-alignment-api"><a class="header" href="#using-the-ripple-vector-alignment-api">Using the Ripple Vector Alignment API</a></h3>
<h4 id="tensors-with-alignment-hint"><a class="header" href="#tensors-with-alignment-hint">Tensors with Alignment Hint</a></h4>
<pre><code class="language-c">#define VECTOR_PE 0

void function_with_aligned_ptrs(size_t size, float *in, float *out) {
    ripple_block_t BS = ripple_set_block_shape(VECTOR_PE, 4);

    ripple_parallel(BS, 0);
    for (size_t i = 0; i &lt; size; ++i) {

        // Indicates that in and out values are aligned to every 4 float values
        *ripple_ptr_alignment(&amp;out[i], 4 * sizeof(float)) = *ripple_ptr_alignment(&amp;in[i], 4 * sizeof(float)) * in[i];

        // Notice that you only need to specify the alignment once for it to apply to expressions of the same value (the second in[i] assumes the same alignment as the first one)
    }
}
</code></pre>
<h4 id="tensors-with-multiple-alignment-hints"><a class="header" href="#tensors-with-multiple-alignment-hints">Tensors with Multiple Alignment Hints</a></h4>
<p>Sometimes you may want to process multiple vectors at once (pairs). To indicate multiple alignment within a tensor, you can use multiple alignment calls by providing the slicing indices:</p>
<pre><code class="language-c">#define VECTOR_PE 0

void function_with_aligned_ptrs(size_t size, float *in, float *out) {
    // Process pairs of 4 values
    ripple_block_t BS = ripple_set_block_shape(VECTOR_PE, 4, 2);

    ripple_parallel(BS, 0, 1);
    for (size_t i = 0; i &lt; size; ++i) {
        // *in* and *out* pointers at tensor indices [0][0] and [0][1] are aligned to 4 float values
        ripple_ptr_alignment(&amp;in[i], 4 * sizeof(float));
        ripple_ptr_alignment(&amp;out[i], 4 * sizeof(float));
        ripple_ptr_alignment_slice(&amp;in[i], 4 * sizeof(float), 0, 1);
        ripple_ptr_alignment_slice(&amp;out[i], 4 * sizeof(float), 0, 1);

        // The 4 alignment hints apply to the following load/store
        out[i] = in[i] * in[i];
    }
}
</code></pre>
<h3 id="using-other-alignment-hints"><a class="header" href="#using-other-alignment-hints">Using Other Alignment Hints</a></h3>
<p>Ripple is based on clang, which natively supports a general alignment mechanism, through the <code>__builtin_assume_aligned()</code> function.
This function lets us indicate that some pointers are aligned on a certain number of bytes.
The compiler uses uses said indications to infer alignment of vector loads and stores occurring as a result of using Ripple.</p>
<p><strong>In certain conditions, these hints cannot effectively be used by the compiler. We encourage to use the <code>ripple_ptr_aligned</code> API as close to the load/store instructions as possible for best results</strong>.</p>
<p>The following example illustrates the use of <code>__builtin_assume_aligned</code>, where we indicate that <code>a</code>, <code>b</code>, and
<code>c</code> are aligned on a 128-byte boundary.
The compiler is capable to calculate that, given the 128-byte alignments, all the vector loads from <code>a</code> and <code>b</code> and the vector store to <code>c</code> in the <code>i</code> loop will be aligned as well.</p>
<pre><code class="language-C">#define VECTOR 0
void vadd(int16_t * c, int16_t * a, int16_t * b, size_t n) {
  a = (int16_t *) __builtin_assume_aligned(a, 128);
  b = (int16_t *) __builtin_assume_aligned(b, 128);
  c = (int16_t *) __builtin_assume_aligned(c, 128);
  ripple_block_t BS = ripple_set_block_shape(0, 64);
  ripple_parallel(BS, 0);
  for (size_t i = 0; i &lt; n; ++i) {
    c[i] = a[i] + b[i];
  }
}
</code></pre>
<h2 id="register-reuse"><a class="header" href="#register-reuse">Register reuse</a></h2>
<p><strong>Performance impact</strong>: High.</p>
<p>Register reuse is about avoiding unnecessary loads and stores
between memory and the registers.
If we need to use data (say a vector X of data) several times in our program,
and we can perform all the computations that use the data right after
loading X from memory to the registers,
then we know that X won’t need to be
evicted from the registers to make room for other values,
and hence X won’t need to be loaded again.
Loads (and stores) are expensive
(they have a much higher access latency than the registers),
hence by not having those extra loads, we are saving time, i.e.,
optimizing our program.</p>
<p>Because Ripple’s underlying compiler (LLVM) manages registers for us,
it is hard to precisely control register use.
The compiler may introduce loads and stores
when it cannot allocate more registers at once,
and conversely, it is able to remove superfluous loads and stores from
an input program in some cases.
A good general heuristic to keep data in registers is to decompose
our computations into
work subsets that don’t use more registers than provided by the hardware.
In the realm of loop transformations, register tiling and loop fission are known
to reduce the number of registers consumed by each work subset.</p>
<h1 id="forcing-vector-execution"><a class="header" href="#forcing-vector-execution">Forcing vector execution</a></h1>
<p><strong>Performance impact</strong>: Low.</p>
<p>Processor-specific compiler backends vary in how they lower vector code
to instructions.
When only a fraction of the vector is utilized, some backends decide to express
the vector computation as a sequence of scalar computations.
As a consequence, performance may be less predictable in partial-vector
computations than with full-vector computations.
The following flag can <strong>add predictability</strong> to Ripple by explicitly expressing
computations as full-vector.</p>
<pre><code class="language-bash">$clang -mllvm -ripple-pad-to-target-simd
</code></pre>
<hr>
<p><em>Copyright (c) 2024-2025 Qualcomm Innovation Center, Inc. All rights reserved.
SPDX-License-Identifier: BSD-3-Clause-Clear</em></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="coalescing-tips"><a class="header" href="#coalescing-tips">Coalescing tips</a></h1>
<p><strong>Performance impact</strong>: High.</p>
<p>In this section, we look at various ways to obtain coalesced (i.e., stride-one access) loads from memory and stores to memory.
Coalescing often has a drastic impact on the performance of the
loads and stores in a Ripple program.</p>
<h1 id="make-clearly-linear-access-functions"><a class="header" href="#make-clearly-linear-access-functions">Make clearly linear access functions</a></h1>
<p>In order to optimize vector loads and stores, the compiler performs
a static analysis of the access patterns.
When the analysis detects memory access patterns that depend linearly upon
the Ripple indices,
it is able to analyze memory strides along all block dimensions.
Whenever stride-1 accesses are detected
(even if it’s along only one of the dimensions),
the compiler generates coalesced vector loads or stores.</p>
<pre><code class="language-C">x = A[...][...][ripple_id(BS, 0)]; // Coalesced access of A
</code></pre>
<p>The general user guidance here is that whenever a coalesced access exists,
make it clear to the compiler in the access functions.
The purpose of this section is to show common pitfalls, in which coalescing is
not achieved, and ways to work around it.</p>
<h2 id="avoid-dividing-ripple_id"><a class="header" href="#avoid-dividing-ripple_id">Avoid dividing <code>ripple_id()</code></a></h2>
<p>Since the vector lane coordinates are integers, using a division results in
an integer division.</p>
<p>Integer divisions are non-linear operations, hence they should not be used
on a Ripple index.
Using an integer division results in the computation of a vector of addresses,
which is then used in a scatter/gather operation.
These are much more expensive than standard vector loads/stores.</p>
<p>For example, consider vectorizing the following piece of sequential code,
where we know that iterations of <code>w</code> are independent.
We store every element of <code>a</code> in <code>y</code>,
and the even elements of <code>b</code>, for every two consecutive iterations, in <code>z</code>.</p>
<pre><code class="language-C">uint8_t foo(int W, uint8_t a[restrict 1][W], uint8_t b[restrict 1][W]) {
  for (int w = 0; w &lt; W; ++w) {
    uint8_t y = a[w];
    uint8_t z = b[2 * (w / 2)];
    // Some other code below
    ...
  }
</code></pre>
<p>To vectorize along <code>w</code>, we take chunks of <code>nv0</code> iterations of <code>w</code>
and map the chunk elements to <code>v0</code>.
Basically, we create a loop (let’s call it <code>u</code>) that goes over the chunks,
and map the values of <code>w</code> within chunks to <code>v0</code>.
To occupy one vector with 8-bit element computations,
let’s start with a one-dimensional block of size 128.</p>
<pre><code class="language-C">uint8_t foo(int W, , uint8_t a[restrict 1][W], uint8_t b[restrict 1][W]) {
  ripple_block_t BS = ripple_set_block_shape(0, 128);
  size_t v0 = ripple_id(BS, 0);
  size_t nv0 = ripple_get_block_size(BS, 0);
  for (int u = 0; u &lt; W; u += nv0) {
    uint8_t y = a[u + v0];
    uint8_t z = b[2 * ( (u + v0) / 2)];
    // Some other code below
    ...
  }
</code></pre>
<p>We find ourselves with</p>
<ul>
<li>a contiguous access for <code>a[u + v0]</code>,</li>
<li>but an access with an integer division in <code>b[2 * ((u + v0)/2)]</code>.
This latter access is very slow and it should be avoided if possible,
because it results in a gather (the most general but slowest kind of load).</li>
</ul>
<p>Returning to the original function,
one fairly easy way to get rid of this integer division in the reference to <code>b</code>
is to strip-mine <code>w</code>.
We basically expose the <code>w/2</code> expression as a loop counter by rewriting the loop
with <code>w = 2*w_o + w_i</code>, and <code>0 &lt;= w_i &lt; 2</code>, resulting in the following code:</p>
<pre><code class="language-C">for (int w_o = 0; w_o &lt; W / 2; ++w_o) {
  for (int w_i = 0; (w_i &lt; 2) &amp; (2 * w_o + w_i &lt; W); ++w_i) {
    uint8_t y = a[2 * w_o + w_i];
    uint8_t z = b[2 * w_o  + (w_i / 2)];
    // Some other code below
    ...
  }
}
</code></pre>
<p>Since <code>w_i / 2</code> is always equal to 0, we can simplify this as:</p>
<pre><code class="language-C">for (int w_o = 0; w_o &lt; W / 2; ++w_o) {
  for (int w_i = 0; (w_i &lt; 2) &amp; (2 * w_o + w_i &lt; W); ++w_i) {
    uint8_t y = a[2 * w_o + w_i];
    uint8_t z = b[2 * w_o];
    // Some other code below
    ...
  }
}
</code></pre>
<p>Now, if we map <code>v0</code> along <code>w_o</code>,
we have stride-2 accesses for <code>a[2 * (w_o  + w_i / 2)]</code> and <code>b[2 * w_o + w_i]</code>
(because of the “2” factor for <code>w_o</code>):</p>
<p>Small-stride loads and stores are cheaper than scatter/gathers</p>
<pre><code class="language-C">ripple_block_t BS = ripple_set_block_shape(0, 128);
size_t v0 = ripple_id(BS, 0);
size_t nv0 = ripple_get_block_size(BS, 0);
for (int u_o = 0; u_o &lt; W / 2; u_o += nv0) {
  for (int w_i = 0; w_i &lt; 2 &amp; (2 * u_o + w_i &lt; W); ++w_i) {
    uint8_t y = a[2 * u_o + 2 * v0 + w_i];
    uint8_t z = b[2 * u_o + 2 * v0];
    // Some other code below
    ...
  }
}
</code></pre>
<p>We can further improve the situation by moving to a 2-dimensional grid.
In order to get contiguous memory access for <code>a</code>,
we can map <code>w_i</code> to the contiguous dimension <code>v0</code>,
and map <code>w_o</code> (previously mapped to <code>v0</code>) to <code>v1</code>.
Since <code>w_i</code> only takes values 0 and 1, a useful block size along <code>v0</code> is 2.
Since <code>v0</code> takes all values of <code>w_i</code>, the <code>w_i</code> loop “disappears”,
leaving only the <code>(2 * u_o + w_i &lt; W)</code> conditional:</p>
<pre><code class="language-C">ripple_block_t BS = ripple_set_block_shape(0, 2, 64);
size_t v0 = ripple_id(BS, 0);
size_t v1 = ripple_id(BS, 1);
size_t nv1 = ripple_get_block_size(BS, 1);
for (int u_o = 0; u_o &lt; W / 2; u_o += nv1) {
  if (2 * u_o + v0 &lt; W) {
    uint8_t y = a[2 * u_o + 2 * v1 + v0];
    uint8_t z = b[2 * u_o + 2 * v1];
    // Some other code below
    ...
  }
}
</code></pre>
<p><code>a</code> is now accessed contiguously, since <code>2 * v1 + v0</code> corresponds to the way
lanes are laid out in a full vector.
Note that we load a <code>2x64</code> tensor out of <code>a</code>, and a <code>1x64</code> tensor out of <code>b</code>.
Operations in the “code below” section that involve both <code>a</code> and <code>b</code> will
trigger a broadcast of <code>b</code> to a <code>2x64</code> shape.</p>
<h1 id="small-stride-loads-and-stores"><a class="header" href="#small-stride-loads-and-stores">Small-stride loads and stores</a></h1>
<p>Strided loads (stores, <em>mutatis mutandis</em>) are typically slow,
since they can require up to as many loads (stores)
as the number of elements in the strided operation.
They will happen for instance if we distribute vector lanes
along columns of a tensor,
since the elements of a column are separated by a full row.
In this case, we recommend either changing the vectorization strategy,
or modifying the tensor’s layout.</p>
<p>Besides choosing a better way to vectorize our code, we can optimize strided
loads (stores) when the strides between elements or chunks are smaller than
a vector-width (e.g. 128 bytes on HVX).
Developers working with complex numbers, coordinate systems and RGB images often
find themselves handling large vectors of tuples,
from which they often need to extract one element at a time (for each tuple).</p>
<p>For instance, if we have a vector of complex numbers,
and we want to take its norm,
we could naively load the real and imaginary parts, as follows:</p>
<pre><code class="language-C">float norm(float vec[32][2]) {
  ripple_block_t BS = ripple_set_block_shape(VECTOR, 32);
  size_t v = ripple_id(BS, 0);
  float real = vec[v][0];
  float imag = vec[v][1];
  return ripple_reduceadd(0b1, real * imag);
}
</code></pre>
<p>The problem with the above code is that the loads of <code>vec[v][0]</code> and <code>vec[v][1]</code>
are both strided loads, which are generally inefficient.
To simplify,
let’s assume that our machine’s hardware vector can contain 32 floats.
A more efficient approach than the strided loads is to load the whole <code>vec</code>
into two registers, and <em>then</em> rearrange the data inside the registers,
as follows:</p>
<pre><code class="language-C">/// First half comes from the odd indices, second half from the even ones.
size_t separate_re_im(size_t i, size_t n) {
    return (i &lt; n / 2) ? 2 * i : 2 * (i - n / 2) + 1;
}

float norm(float vec[32][2]) {
  ripple_block_t BS = ripple_set_block_shape(VECTOR, 32, 2);
  size_t v0 = ripple_id(BS, 0);
  size_t v1 = ripple_id(BS, 1);
  float all_data = vec[v0][v1];
  float real_and_im = ripple_shuffle(all_data, separate_re_im);
  return ripple_reduceadd(0b01, ripple_reducemul(0b10, real_and_im));
}
</code></pre>
<p>Thus, with 4 additional lines of code, we can go down from 64 loads
(assuming each scalar in the strided loads have to be loaded separately)
to 2 (vector) loads.</p>
<p>In this example, we could also have used the <code>shuffle_pair</code> interface to stay closer to the original idea to have a <code>real</code> and
<code>imag</code> blocks, as follows:</p>
<pre><code class="language-C">size_t take_real_from_pair(size_t i, size_t n) {
  return 2 * i;
}

size_t take_imag_from_pair(size_t i, size_t n) {
  return 2 * i + 1;
}

float norm(float vec[32][2]) {
  ripple_block_t BS = ripple_set_block_shape(VECTOR, 32);
  size_t v0 = ripple_id(BS, 0);
  size_t v1 = ripple_id(BS, 1);
  float all_data = vec[v0][v1];
  float first = *(vec + v0);
  float second = *(vec + 32 + v0);
  float real =
    ripple_shuffle_pair(first, second, take_real_from_pair);
  float imag =
    ripple_shuffle_pair(first, second, take_imag_from_pair);
  return ripple_reduceadd(0b01, real * imag);
}
</code></pre>
<p>To do so, we had to use two different (but simpler) shuffle functions.</p>
<p>In C++, the <code>ripple/zip.h</code> header provides a few functions
to help load and store vectors of fixed-size tuples and turn them
into tuples of vectors.
In particular, the <code>rzip::shuffle_unzip</code> shuffle function
can express all the necessary in-vector data reordering.</p>
<h1 id="data-reordering"><a class="header" href="#data-reordering">Data reordering</a></h1>
<p>Some famous data layout reorderings, like data tiling,
can speed up algorithms by laying out data
that are used together (a tile) consecutively in memory.
That way, a single tile can be loaded with a small number of coalesced loads.</p>
<h2 id="tradeoff"><a class="header" href="#tradeoff">Tradeoff</a></h2>
<p>When considering data reordering in the process of optimizing a program,
it is important to consider the cost of the data reorganization itself,
as they can involve a lot of loads and stores.
If the data reordering does not result in enough gain
to offload the cost of the data reordering,
then the reordering is not worthwhile.</p>
<hr>
<p><em>Copyright (c) 2024-2025 Qualcomm Innovation Center, Inc. All rights reserved.
SPDX-License-Identifier: BSD-3-Clause-Clear</em></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hexagon-r-hvx-optimization"><a class="header" href="#hexagon-r-hvx-optimization">Hexagon (R) HVX Optimization</a></h1>
<p>This section is structured as a list of coding recommendations
in order to get competitive performance out of your Ripple programs.</p>
<h1 id="optimization-level"><a class="header" href="#optimization-level">Optimization level</a></h1>
<p>Ripple has been most tested with the <code>-O2</code> option on HVX,
which seems to be the most common optimization level.</p>
<pre><code class="language-bash">clang -O2 ...
</code></pre>
<h1 id="vector-size-parameters"><a class="header" href="#vector-size-parameters">Vector size parameters</a></h1>
<p>Current versions of HVX ISAs tend to come with a register file of 32Kb total.
Please check the appropriate programmer’s manuals for any specific information,
such as number and bit-width of registers,
about the particular architecture version you are compiling for.
The following block sizes illustrate single-vector block sizes
assuming a vector width of 1024 bits.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>element type</th><th>i8/u8</th><th>i16/u16/f16</th><th>i32/u32/f32</th></tr>
</thead>
<tbody>
<tr><td>single-vector block size</td><td>128</td><td>64</td><td>32</td></tr>
</tbody>
</table>
</div>
<p>Since HVX instruction packets can contain up to four instructions, it is usually not useful to make the size of your block larger than four vectors.
For more information about VLIW slot usage, please consult the Hexagon HVX Programmer’s Reference Manual (available on <code>docs.qualcomm.com</code>).</p>
<h1 id="make-the-type-of-constants-immediates-explicit"><a class="header" href="#make-the-type-of-constants-immediates-explicit">Make the type of constants (“immediates”) explicit</a></h1>
<p><strong>Performance impact</strong>: High.</p>
<p>One thing to remember when coding in Ripple, is that it maintains all semantical aspects of its underlying language.
One aspect of C that can impact performance in C and C++ is their <strong>implicit type conversions</strong>, and the <strong>default type</strong> for its constants.</p>
<p><strong>Example 1.</strong> Consider the following function, which doubles the value of a <code>float</code> array.</p>
<pre><code class="language-c">1:void double_me(float * A, unsigned n) {
2:  ripple_block_t b = ripple_set_block_shape(HVX_LANES, 32);
3:  ripple_parallel(b, 0);
4:  for (int i = 0; i &lt;n; ++i) {
5:    A[i] = 2.0 * A[i];
6:  }
7:}
</code></pre>
<p><strong>Problem</strong>: The <code>2.0</code> immediate is by default a <code>double</code> in C and C++.
Also, type promotion rules in C/C++ indicate that the multiplication line 5 is done after promoting both operands to the same type, <code>double</code>.
Hence, Ripple tries to produce a SIMD multiplication of 32 doubles.
However, current versions of HVX do not support SIMD
double-precision instructions.
As a result, the generated code is sequentialized, resulting in an order of magnitude slower performance than if SIMD could have been used.</p>
<p><strong>Solution</strong>: Make the <code>2.0</code> immediate explicitly of the <code>float</code> type, using the <code>f</code> suffix.</p>
<pre><code class="language-c">5:     A[i] = 2.0f * A[i];
</code></pre>
<p>The same issue can happen with integers, for which the default type is <code>int</code>, as in the following example:</p>
<pre><code class="language-c">1:void double_me(short * A, unsigned n) {
2:  ripple_block_t b = ripple_set_block_shape(HVX_LANES, 64);
3:  ripple_parallel(b, 0);
4:  for (int i = 0; i &lt;n; ++i) {
5:    A[i] = 2 * A[i];
6:  }
7:}
</code></pre>
<p>Here, the <code>2</code> line 5 is an <code>int</code>, and so is the subsequent multiplication,
because of type promotion rules in the C programming language.
<strong>Solution</strong>: convert it to short explicitly.</p>
<pre><code class="language-c">5:    A[i] = ((short) 2) * A[i];
</code></pre>
<p>If you are using C++, precomputing it and forcing it to be a constexpr
will make sure that there is no extra cost to this conversion, as in the following code:</p>
<pre><code class="language-c++">1:void double_me(short * A, unsigned n) {
2:  ripple_block_t b = ripple_set_block_shape(HVX_LANES, 64);
3:  constexpr short two = (short) 2;
3:  ripple_parallel(b, 0);
4:  for (int i = 0; i &lt;n; ++i) {
5:    A[i] = two * A[i];
6:  }
7:}
</code></pre>
<h1 id="optimizing-floating-point-code"><a class="header" href="#optimizing-floating-point-code">Optimizing floating-point code</a></h1>
<h2 id="use-floating-point-types-up-to-32-bit-wide-float-only"><a class="header" href="#use-floating-point-types-up-to-32-bit-wide-float-only">Use floating point types up to 32-bit wide (<code>float</code>) only</a></h2>
<p><strong>Performance impact</strong>: High.</p>
<p>Hexagon supports two native floating-point types: float and _Float16.
__bf16 is partially emulated.
There isn’t currently a native HVX <code>double</code> vector ISA.
Hence, avoid double-precision computations in your vector codes,
which will come out as sequentialized.</p>
<h2 id="leverage-compiler-flags"><a class="header" href="#leverage-compiler-flags">Leverage compiler flags</a></h2>
<h3 id="extended-precision"><a class="header" href="#extended-precision">Extended precision</a></h3>
<p><strong>Performance impact</strong>: Medium.</p>
<p>HVX allows extended-precision computations to happen in sequences of floating-point operations happening between memory accesses.
These are in a way more precise than IEEE floating-point, but they will result in a different numerical result from the same computation performed on IEEE floating point numbers.
It is possible to forgo of extended precision and be fully compatible with IEEE. However, this comes at a performance cost.</p>
<p>The use of fast, extended precision vs. slower, IEEE-compliant precision, is controlled by the following flag:</p>
<pre><code class="language-bash">-mhvx-qfloat=&lt;mode&gt;
</code></pre>
<p>We refer to the HVX manual to describe the exact behavior of the following four modes:</p>
<ul>
<li><code>strict-ieee</code> will give you full compatibility with IEEE floating point.</li>
<li><code>ieee</code> uses extended precision for some operations.</li>
<li><code>lossy</code> uses extended precision for more operations.</li>
<li><code>legacy</code> uses extended precision for almost all operations.</li>
</ul>
<p>As of clang 21.0, the default is set to <code>lossy</code>.
<strong>Warning</strong>: the <code>lossy</code> option also implies other flags that impact precision, such as <code>-ffast-math</code>, which declares that floating-point (+, *) operations are commutative and associative, and that the data is free of <code>NaN</code>s and infinity.</p>
<h3 id="working-with-finite-values"><a class="header" href="#working-with-finite-values">Working with finite values</a></h3>
<p><strong>Performance impact</strong>: Low.</p>
<p>clang is able to optimize some codes when you know that your floating-point values are always defined (they are not a NaN) and never infinite.
If you are in that case, use the <code>-ffinite-only</code> command-line flag.</p>
<h3 id="relaxing-the-ordering-of-computations"><a class="header" href="#relaxing-the-ordering-of-computations">Relaxing the ordering of computations</a></h3>
<p><strong>Performance impact</strong>: Medium.</p>
<p>Addition and multiplication are commutative and associative for real numbers. This means that you can change the order in which a sequence of (for instance) additions (often called “reductions”) are computed without changing the result.
Being able to change this order allows compilers to execute such sequences of operations faster.</p>
<p>Floating point numbers are an approximation for real numbers, and as a consequence, their operations aren’t associative strictly speaking.
Changing the order of floating-point computations typically introduces some amount of precision error.</p>
<p><strong>Solution</strong> If your computation tolerates some amount of floating-point error, or if semantically that order does not matter for the resulting computation, you can let Ripple know it, using the following command-line flag:</p>
<pre><code class="language-bash">clang -fassociative-math
</code></pre>
<p>This will speed up some computations, including reductions (expressed through the <code>ripple_reduce_*()</code> ripple API).</p>
<h3 id="fast-conversions-from-floating-point-to-integer"><a class="header" href="#fast-conversions-from-floating-point-to-integer">Fast conversions from floating-point to integer</a></h3>
<p><strong>Performance impact</strong>: Low.</p>
<p>Conversion from floating-point (<code>Float</code> and <code>float</code> types) to integer
(<code>int16_t</code> and <code>int32_t</code>) can be slow,
as clang tries to implement it as precisely as possible.</p>
<p><strong>Solution</strong> Enable fast floating-point conversion,
which is very close semantically, but uses a single instruction.
To enable it, use the following command-line flag:</p>
<pre><code class="language-bash">clang -mllvm -hexagon-fp-fast-convert=true ...
</code></pre>
<h1 id="using-ripple_shuffle-on-vector-pairs"><a class="header" href="#using-ripple_shuffle-on-vector-pairs">Using ripple_shuffle on vector pairs</a></h1>
<p><strong>Performance impact</strong>: Medium.</p>
<p>We empirically noticed that shuffles on blocks of 2 vectors tend to
generate more efficient machine code.</p>
<h1 id="inlining-functions"><a class="header" href="#inlining-functions">Inlining functions</a></h1>
<p><strong>Performance impact</strong>: High.</p>
<p>For an HVX target, there are significant overheads associated with function calls.
The overheads mostly arise from saving the register frame onto the stack
before the function call and loading the register frame from the stack
after the call returns.
Consequently, we advise the programmer to utilize clang’s
<a href="https://clang.llvm.org/docs/AttributeReference.html#always-inline-force-inline">“always_inline”</a>
function attribute.
Avoid these overheads while maintaining code-readability as follows:</p>
<pre><code class="language-C"> static int foo(int a, int b) __attribute__((always_inline)) {
    /// ...
  }
</code></pre>
<p>or in C++</p>
<pre><code class="language-C++">static int foo(int a, int b) [[gnu::always_inline]] {
    /// ...
  }
</code></pre>
<h1 id="functions-optimized-for-hvx"><a class="header" href="#functions-optimized-for-hvx">Functions optimized for HVX</a></h1>
<h2 id="high-throughput-data-reordering-scattergather"><a class="header" href="#high-throughput-data-reordering-scattergather">High-throughput data reordering (scatter/gather)</a></h2>
<p><strong>Performance impact</strong>: High.</p>
<p>Loading from a collection of arbitrary addresses into a vector
is by construction a long-latency operation,
as it can result in bank conflicts.
Hence, we do not recommend the direct use of non-coalesced access functions.</p>
<p>However, the Ripple vector library includes a
high-bandwidth memory-to-memory copying API for HVX,
which can move large amounts of data in parallel
(still with the same type of latency).</p>
<p>In cases where we cannot change our computations
to create coalesced data accesses, we can still rearrange our data into a set
that can be accessed in a coalesced way a bit later,
using the <code>hvx_gather/hvx_scatter</code> API.</p>
<p>Use cases for scatter-gather include indirections (<code>A[B[i]]</code>),
large lookup tables, sparse-dense array computations (for instance sparse matrix dense vector multiplication), and strided data accesses, particularly when the data access stride is large (e.g. when accessing element of a large array along columns).
We refer to the Ripple manual for a more complete description of
<code>hvx_scatter</code> and <code>hvx_gather</code>.</p>
<h2 id="explicit-bfloat16-conversions"><a class="header" href="#explicit-bfloat16-conversions">Explicit bfloat16 conversions</a></h2>
<p><strong>Performance impact</strong>: High.</p>
<p>Ripple supports three types of conversions from <code>float</code> to <code>__bf16</code>:</p>
<ul>
<li><code>to_bf_trunc</code>, a direct truncation</li>
<li><code>to_bf_nan</code>, a truncation that avoids incorrect NaN conversions (they can become an infinity using the direct shift)</li>
<li><code>to_bf_round</code>, a conversion that performs an even rounding and avoids incorrect Nan conversions. The behavior of this conversion is compatible with x86 bfloat conversion.</li>
</ul>
<h2 id="dynamic-rotations"><a class="header" href="#dynamic-rotations">Dynamic rotations</a></h2>
<p><strong>Performance impact</strong>: Low.</p>
<p><code>hvx_rotate(x, n)</code> will consider x flat (irrespective of its shape) and rotate the elements towards n lower indices
(e.g. the element at position 3+n goes to position 3).
One valuable aspect of this function is that <code>n</code> doesn’t need to be
a compile-time constant.
<code>n</code> could be a loop counter, for instance.</p>
<h2 id="narrowing-shifts"><a class="header" href="#narrowing-shifts">Narrowing shifts</a></h2>
<p><strong>Performance impact</strong>: Medium.</p>
<h3 id="syntax"><a class="header" href="#syntax">Syntax</a></h3>
<pre><code class="language-C">narrow_t hvx_narsh[[_rnd]_sat][_noshuff](wide_t odd, wide_t even, uint32 shift);
</code></pre>
<p>A narrowing shift (or “narsh”) is an operation on integers,
which performs a right shift followed by a conversion
to a smaller integer type.
The conversion can be done by truncation or using saturation (using the function name suffix <code>_sat</code>).
When saturation is used, we can also request rounding up, using the
<code>_sat_rnd</code> suffix.
The shift amount sh is the same for all elements of the block.</p>
<p>Two forms are supported. The first, faster form takes two inputs, performs a narrowing shift, and shuffles the resulting block.
The second form, suffixed with <code>_noshuff</code>, doesn’t shuffle the result.</p>
<h3 id="supported-source-and-destination-types-__"><a class="header" href="#supported-source-and-destination-types-__">Supported source and destination types __</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>wide (source) type</th><th>narrow (destination) type</th></tr>
</thead>
<tbody>
<tr><td>i32 (int32_t)</td><td>i16, u16</td></tr>
<tr><td>u32 (uint32_t)</td><td>u16</td></tr>
<tr><td>i16 (int16_t)</td><td>i8, u8</td></tr>
<tr><td>u16</td><td>u8</td></tr>
</tbody>
</table>
</div>
<p>An interesting aspect of this function is that it packs
the narrowed by pairs, i.e. two 32-bit integers get narrowed and packed into one 32-bit integer.</p>
<h3 id="examples"><a class="header" href="#examples">Examples</a></h3>
<pre><code class="language-C">int16_t hvx_narsh_sat_i16toi8(int16_t odd, int16_t even, uint32_t shift);
uint32_t hvx_narsh_rnd_sat_u32tou16_noshuff(uint32_t odd, uint32_t even, uint32_t shift);
</code></pre>
<h3 id="constraints"><a class="header" href="#constraints">Constraints</a></h3>
<p>hvx_narsh works on block sizes that occupy a full HVX vector.</p>
<h2 id="vector-splicing"><a class="header" href="#vector-splicing">Vector splicing</a></h2>
<p><strong>Performance impact</strong>: High.</p>
<h3 id="syntax-1"><a class="header" href="#syntax-1">Syntax</a></h3>
<pre><code class="language-C">T hvx_splice(T high, T low, size_t n);
T hvx_lsplice(T high, T low, size_t n);
</code></pre>
<p>This is a <em>flat</em> operator, which means that it considers a block as being
one-dimensional.
Hence, we describe blocks here as “vector“s.
<code>hvx_splice</code> forms a vector from two vectors <code>high</code> and <code>low</code>.
Let’s call <code>N</code> the number of elements in the vector.
The returned vector contains
the <code>n</code> lowest elements of <code>high</code> as its highest elements,
and the <code>N-n</code> highest elements of <code>low</code> as its lowest elements.</p>
<p><code>hvx_lsplice</code> (“low splice”) forms a vector from two vectors <code>high</code> and <code>low</code>.
The returned vector contains
the <code>n</code> highest elements of <code>low</code> as its lowest elements,
and the <code>N-n</code> lowest elements of <code>high</code> as its highest elements.</p>
<p>This behavior is illustrated on Figure H1.
<img src="hvx_splice.png" title="hvx_splice behavior" alt="hvx_splice and hvx_lsplice function behavior">
<strong>Figure H1.</strong> hvx_splice/hvx_lsplice behavior</p>
<p><strong>Note</strong>: An implicit “modulo N” is applied to <code>n</code>, to ensure sound semantics.</p>
<h3 id="examples-1"><a class="header" href="#examples-1">Examples</a></h3>
<p>Typical uses for <code>hvx_splice</code> are codes that use overlapping windows of tensors,
for example in stencils, convolutions and pooling.
Example 2 will be a sliding window, a 1-dimensional Gaussian blur.</p>
<p><code>hvx_splice</code> is also sometimes used to implement padding, as shown in Example 1.</p>
<p><strong>Example 1</strong>: We need to add two arrays, but only one of them has a size
that is a multiple of the HVX vector size (here called <code>HVX_SIZE_U16</code>).</p>
<pre><code class="language-C++">#define HVX_SIZE_U16 64
void padded_add(size_t n, uint16_t * in1_padded, uint16_t * in2_unpadded,
                uint16_t * out_padded) {
  ripple_block_t B = ripple_set_shape(HVX_LANES, HVX_SIZE_U16);
  size_t v = ripple_id(B, 0);
  size_t i;
  // full-vector loop: we know we have full vectors from in1 and in2
  for (i = 0; i &lt; n; i += HVX_SIZE_U16) {
    out_padded[i + v] = in1_padded[i + v] + in2_unpadded[i + v];
  }
  // epilogue: There aren't enough elements in in2_unpadded to add with.
  //           We're splicing that partial vector with a vector of zeros
  out_padded[i + v] = in1_padded[i + v] +
    // we want to specify the #elements taken from the lower vector --&gt; lsplice
                      hvx_lsplice(0, in2_unpadded[i + v], n - i);
}
</code></pre>
<p>Note that the same effect can be obtained using conditionals in this case.
The epilogue code in <code>padded_add</code> below could be replaced with the following
code to obtain the same result.</p>
<pre><code class="language-C++">  out_padded[i + v] = in1_padded[i + v] + (i + v &lt; n) ? in2_unpadded[i + v] : 0;
</code></pre>
<p>The difference between the two lies in the use of predicate registers
in the latter.
The sequence of instructions for the original padded_add epilogue are roughly:</p>
<ul>
<li>Compute <code>n - i</code> (a scalar).</li>
<li>Top off <code>in2_unpadded</code> with zeros using <code>hvx_lsplice</code>.</li>
<li>Add <code>in1_padded</code> with the combined vector</li>
</ul>
<p>The sequence of instruction for the conditional-based version is roughly:</p>
<ul>
<li>Compute the vector predicate for the expression <code>i + v &lt; n</code>.</li>
<li>Select the elements from the zero-vector and <code>in2_unpadded</code>
as specified in the vector predicate.</li>
<li>Add <code>in1_padded</code> with the resulting vector</li>
</ul>
<p>While the number of steps seems comparable,
computing predicates can be more expensive than computing non-predicates.
Hence in this case, the version based on <code>hvx_lsplice</code> is generally preferred.</p>
<p><strong>Example 2</strong>: Stencil operations are often used to write filters,
or iteratively solve partial differential equations.</p>
<p>The following code is a simple gaussian blur operating on a 1-d signal:</p>
<pre><code class="language-C++">void gaussian_blur(size_t n, float * in, float * out) {
  constexpr float one_third = 1.f/3.f;
  for (size_t i = 1; i &lt; n - 1; ++i) {
    out[i] = (in[i - 1] + in[i] + in[i + 1]) * one_third;
  }
}
</code></pre>
<p>A naïve vectorization can be obtained by decorating the <code>i</code> loop with
<code>ripple_parallel</code>.
No matter how <code>in</code> and <code>out</code> are align, at least two of the loads of <code>in</code>
will be unaligned.
We can do better by using only aligned loads and stores.</p>
<pre><code class="language-C++">void gaussian_blur(size_t n, float * in, float * out) {
  constexpr float one_third = 1.f/3.f;
  constexpr size_t nv = 32;
  constexpr size_t hvx_bytes = nv * sizeof(float);
  ripple_block_t B = ripple_set_shape(HVX_LANES, nv);
  size_t v = ripple_id(B, 0);
  float * in_ptr_lower, * in_ptr, * in_ptr_upper;

  // prologue
  in_ptr = ripple_ptr_alignment(&amp;in[0], hvx_bytes);
  in_ptr_upper = ripple_ptr_alignment(&amp;in[nv], hvx_bytes);
  float * out_ptr = ripple_ptr_alignment(&amp;out[i], hvx_bytes);
  out_ptr[v] = (hvx_lsplice(0, in_ptr, 1) +
               in_ptr[v] +
               hvx_lsplice(in_ptr, in_ptr_upper, 1)) * one_third;

  // steady-state loop
  size_t i;
  for (i = nv; i &lt; n - nv; i += nv) {
    in_ptr = ripple_ptr_alignment(&amp;in[i], hvx_bytes);
    in_ptr_lower = ripple_ptr_alignment(&amp;in[i - nv], hvx_bytes);
    in_ptr_upper = ripple_ptr_alignment(&amp;in[i + nv], hvx_bytes);
    float * out_ptr = ripple_ptr_alignment(&amp;out[i], hvx_bytes);

    out_ptr[v] = // take one extra element from the lower vector
                 (hvx_lsplice(in_ptr_lower, in_ptr, 1) +
                  in[v] +
                  // take one extra element from the upper vector
                  hvx_splice(in_ptr, in_ptr_upper, 1)) * one_third;
  }

  // epilogue
  if (i + v &lt; n) {
    in_ptr = ripple_ptr_alignment(&amp;in[i], hvx_bytes);
    in_ptr_lower = ripple_ptr_alignment(&amp;in[i - nv], hvx_bytes);
    float * out_ptr = ripple_ptr_alignment(&amp;out[i], hvx_bytes);

    out_ptr[v] = (hvx_lsplice(in_ptr_lower, in_ptr, 1) +
                  in_ptr[v] +
                  hvx_lsplice(in_ptr, 0, 1)) * one_third;
  }
}
</code></pre>
<p>The code above is faster than the original code,
because it only involves aligned loads and stores.</p>
<p>We can make it even faster by noticing that consecutive iterations of <code>i</code>
reuse two of the three input vectors.
Hence, instead of re-loading these two vectors,
we can just reuse them from the previous iteration.
This reuse technique, often called “register rotation,”
is applied to <code>gaussian_blur</code> below.</p>
<pre><code class="language-C++">void gaussian_blur(size_t n, float * in, float * out) {
  constexpr float one_third = 1.f/3.f;
  constexpr size_t nv = 32;
  constexpr size_t hvx_bytes = nv * sizeof(float);
  ripple_block_t B = ripple_set_shape(HVX_LANES, nv);
  size_t v = ripple_id(B, 0);
  float * in_ptr_lower, * in_ptr, * in_ptr_upper;

  // prologue
  in_ptr = ripple_ptr_alignment(&amp;in[0], hvx_bytes);
  in_ptr_upper = ripple_ptr_alignment(&amp;in[nv], hvx_bytes);
  float * out_ptr = ripple_ptr_alignment(&amp;out[i], hvx_bytes);
  out_ptr[v] = (hvx_lsplice(0, in_ptr, 1) +
                in_ptr[v] +
                hvx_lsplice(in_ptr, in_ptr_upper, 1)) * one_third;

  // steady-state loop
  size_t i;
  for (i = nv; i &lt; n - nv; i += nv) {
    // register rotation: in_ptr -&gt; in_ptr_lower; in_ptr_upper -&gt; in_ptr
    in_ptr_lower = ripple_ptr_alignment(in_ptr, hvx_bytes);
    in_ptr = ripple_ptr_alignment(in_ptr_upper, hvx_bytes);
    // in_ptr_upper is new, still need to load it
    in_ptr_upper = ripple_ptr_alignment(&amp;in[i + nv], hvx_bytes);
    float * out_ptr = ripple_ptr_alignment(&amp;out[i], hvx_bytes);

    out_ptr[v] = // take one extra element from the lower vector
                 (hvx_lsplice(in_ptr_lower, in_ptr, 1) +
                  in[v] +
                  // take one extra element from the upper vector
                  hvx_splice(in_ptr, in_ptr_upper, 1)) * one_third;
  }

  // epilogue
  if (i + v &lt; n) {
    // register rotation applied here as well
    in_ptr_lower = ripple_ptr_alignment(in_ptr, hvx_bytes);
    in_ptr = ripple_ptr_alignment(in_ptr_upper, hvx_bytes);
    float * out_ptr = ripple_ptr_alignment(&amp;out[i], hvx_bytes);

    out_ptr[v] = (hvx_lsplice(in_ptr_lower, in_ptr, 1) +
                  in_ptr[v] +
                  hvx_splice(in_ptr, 0, 1)) * one_third;
  }
}
</code></pre>
<p>Let <code>V</code> the number of HVX vectors in <code>in</code>.
With this transformation, the total number of vector loads from <code>in</code> went from
about <code>3*V</code> to <code>V</code>, without adding any work, a clear performance win.
Note that the versions of <code>gaussian_blur</code> above that use <code>hvx_splice</code> assume
that it’s safe to read up to one vector past the upper boundary of <code>in</code>.
It is possible to remove that limitation by writing slightly more complex code.</p>
<h3 id="constraints-1"><a class="header" href="#constraints-1">Constraints</a></h3>
<p><code>high</code> and <code>low</code> shapes must correspond to
the number of elements in one HVX Vector.</p>
<hr>
<p><em>Copyright (c) 2024-2025 Qualcomm Innovation Center, Inc. All rights reserved.
SPDX-License-Identifier: BSD-3-Clause-Clear</em></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="profiling-on-hexagonr"><a class="header" href="#profiling-on-hexagonr">Profiling on Hexagon(R)</a></h1>
<h2 id="counting-cycles-in-the-code"><a class="header" href="#counting-cycles-in-the-code">Counting cycles in the code</a></h2>
<p>The simplest way to measure performance is by reading cycle counters,
using clang’s standard builtin:</p>
<pre><code class="language-C">unsigned long long __builtin_readcyclecounter();
</code></pre>
<p>For instance, the following code measures the cycle count of a 2-d nested loop:</p>
<pre><code class="language-C">#include&lt;ripple.h&gt;
/* We assume the existence of some function float f(float) in this module */
void foo(float * in, float * out, size_t n) {
  ripple_block_t B = ripple_set_block_shape(HVX_LANES, 32);
  unsigned long long start, end;
  start = __builtin_readcyclecounter();
  for (size_t i = 0; i &lt; n; ++i) {
    ripple_parallel(B, 0);
    for (size_t j = 0; j &lt; n; ++j) {
      a[n*i + j] = f(b[n*i + j]);
    }
  }
  end = __builtin_readcyclecounter();
  printf("2-d loop took %llu cycles.\n", end - start);
}
</code></pre>
<h2 id="profiling-influences-performance"><a class="header" href="#profiling-influences-performance">Profiling influences performance</a></h2>
<p>As often, observing a system changes it.
In this section, we share insights about how and when using the profiling API could reduce the performance of the profiled code.</p>
<h3 id="instruction-scheduling-overhead"><a class="header" href="#instruction-scheduling-overhead">Instruction scheduling overhead</a></h3>
<p>While it is tempting to try and profile small portions of code,
we need to consider that the calls to <code>__builtin_readcyclecounter()</code> themselves result in instructions.
As a result, the compiler has to schedule these profiling instructions along with the instructions you want to profile.
This can result in a slightly higher number of VLIW packets,
which will increase the number of cycles used for your computation.</p>
<h3 id="risk-of-performance-degradation-from-instruction-pipelining"><a class="header" href="#risk-of-performance-degradation-from-instruction-pipelining">Risk of performance degradation from instruction pipelining</a></h3>
<p>If we are trying to accumulate cycle counts in a specific region of code (for instance within a loop), we need to consider that fine-grained instruction scheduling can be affected by the presence of profiling code in the inner loop.
For instance, if we wanted to accumulate the cycle counts of the <code>j</code> iterations
in the above examples, we would write the following code:</p>
<pre><code class="language-C">1: void foo(float * in, float * out, size_t n) {
2:   ripple_block_t B = ripple_set_block_shape(HVX_LANES, 32);
3:   unsigned long long start, end, accumulated;
4:   for (size_t i = 0; i &lt; n; ++i) {
5:     #pragma clang loop unroll(2)
6:     ripple_parallel(B, 0);
7:     for (size_t j = 0; j &lt; n; ++j) {
8:       start = __builtin_readcyclecounter();
9:       a[n*i + j] = f(b[n*i + j]);
10:      end = __builtin_readcyclecounter();
11:      accumulated += end - start;
12:    }
13:  }
14:  printf("2-d loop too %llu cycles.\n", accumulated);
15:}
</code></pre>
<p>We note that, as far as Ripple is concerned, this code is correct.
Since <code>accumulated</code> doesn’t depend upon <code>j</code>,
it is interpreted as a scalar value,
and the statement line 11 is also a scalar increment.
The accumulation will happen only once per vector computation
(n/32 times total), which is the intended behavior.</p>
<p>However, the profiling code can reduce the code performance,
compared to the unprofiled code,
by constraining the compiler’s freedom to schedule instructions efficiently.
Hexagon being a VLIW machine, the Hexagon compiler needs to schedule instructions as packets of parallel instructions.
The more instructions can be reordered,
the better the chances for the compiler’s VLIW scheduler
to create well-utilized packets.</p>
<p>In the example above, calls to <code>__builtin_readcyclecounter()</code> cannot be rescheduled w.r.t. other instructions, to let developers measure exactly what they want.
This also means that other instructions cannot go past a call to <code>__builtin_readcyclecounter()</code>. As a result, instructions from line 9 cannot be combined with instructions from other lines.
Instructions from line 9 at a certain <code>j</code> iteration also cannot be combined with instructions from the next <code>j</code> iteration
(such cross-iteration combination is normally enabled by the <code>unroll</code> loop pragma line 5).
Hence, the compiler has much less freedom to pack instructions into VLIW packets as efficiently as with the unprofiled code.</p>
<h2 id="application-profiling-with-itrace"><a class="header" href="#application-profiling-with-itrace">Application profiling with <code>itrace</code></a></h2>
<p>itrace is a low-level profiling tool in the Hexagon SDK that provides:</p>
<ul>
<li>PMU (Performance Monitoring Unit) event capture for DSP and optionally CPU.</li>
<li>Markers and sections for custom instrumentation.</li>
<li>Periodic sampling of counters.</li>
<li>Ability to log traces in Perfetto-compatible formats for visualization.
It’s more flexible than sysmon but requires code integration and configuration.</li>
</ul>
<p>itrace is located in the Hexagon SDK under</p>
<pre><code>libs/itrace/
</code></pre>
<p>To use itrace, lease consult the HTML documentation in the SDK:
<code>/docs/doxygen/itrace/index.html</code></p>
<p>Event IDs are defined in the following files:</p>
<ul>
<li>DSP PMU: itrace_dsp_events_pmu.h</li>
<li>CPU events: itrace_cpu_events_la.h</li>
</ul>
<h3 id="basic-workflow"><a class="header" href="#basic-workflow">Basic workflow</a></h3>
<h4 id="initialize-profiler"><a class="header" href="#initialize-profiler">Initialize profiler</a></h4>
<pre><code class="language-C">itrace_profiler_handle_t dsp_profiler_handle;
itrace_profiler_create(&amp;dsp_profiler_handle, ITRACE_DOMAIN_DSP);

</code></pre>
<h4 id="register-the-pmu-events-you-want-to-see"><a class="header" href="#register-the-pmu-events-you-want-to-see">Register the PMU events you want to see</a></h4>
<pre><code class="language-C">itrace_add_event(dsp_profiler_handle, ITRACE_DSP_EVENT_L1D_CACHE_MISS);
itrace_add_event(dsp_profiler_handle, ITRACE_DSP_EVENT_INSTR_COUNT);
</code></pre>
<h4 id="startstop-profiling"><a class="header" href="#startstop-profiling">Start/stop profiling</a></h4>
<pre><code class="language-C">itrace_start(dsp_profiler_handle);
// Run your workload
itrace_stop(dsp_profiler_handle);
itrace_dump(dsp_profiler_handle, "trace_output.json");
</code></pre>
<p>Output can be JSON or protobuf for Perfetto visualization.</p>
<h3 id="instrumentation"><a class="header" href="#instrumentation">Instrumentation</a></h3>
<p>Add markers for timeline correlation:</p>
<pre><code class="language-C">
itrace_add_marker(dsp_profiler_handle, "Begin DSP section");
itrace_add_section(dsp_profiler_handle, "FFT compute");
// ... code ...
itrace_end_section(dsp_profiler_handle);

</code></pre>
<p>Markers work on CPU and DSP; DSP-side markers give best granularity.</p>
<h3 id="advanced-features"><a class="header" href="#advanced-features">Advanced features</a></h3>
<p><em><strong>Periodic sampler:</strong></em> sample PMU counters at fixed intervals.</p>
<p><em><strong>Multi-pass mode:</strong></em> if you need &gt;8 PMU events, itrace cycles through sets automatically.</p>
<p><em><strong>Perfetto integration:</strong></em> itrace can emit traces compatible with Chrome’s about://tracing or Perfetto UI.</p>
<h3 id="constraints-2"><a class="header" href="#constraints-2">Constraints</a></h3>
<ul>
<li>Only one PMU client at a time (itrace vs sysmon vs SDP).
If you see registration errors, stop other profilers or reboot.</li>
<li>itrace is per-application; unlike sysmon, it doesn’t give system-wide DSP load.</li>
<li>Some SDK versions (e.g., v69/v75) have known itrace bugs — please check release notes.</li>
</ul>
<p><strong>Disclaimer</strong>: <code>sysmon</code> is entirely independent from Ripple,
and Ripple support won’t be able to assist you with <code>sysmon</code> issues.</p>
<h2 id="profiling-whole-dsp-execution-with-sysmon"><a class="header" href="#profiling-whole-dsp-execution-with-sysmon">Profiling whole DSP execution with <code>sysmon</code></a></h2>
<p>Hexagon’s SysMon (“sysMonApp”) is the command‑line profiler bundled with the Hexagon SDK that lets you capture DSP workload/utilization and PMU (Performance Monitoring Unit) counters from different Q6 subsystems (ADSP/CDSP/SDSP/MDSP) across Android, Linux LE, QNX and virtualized (Gunyah) targets. Below is a concise, engineer‑friendly “how to” that you can use directly.</p>
<h3 id="where-to-find-sysmon-in-the-sdk"><a class="header" href="#where-to-find-sysmon-in-the-sdk">Where to find sysmon in the SDK</a></h3>
<p>SysMon binaries are shipped under the SDK install at:</p>
<pre><code class="language-bash">&lt;HEXAGON_SDK_ROOT&gt;/tools/utils/sysmon/
</code></pre>
<p>Typical artifacts include:</p>
<p>sysMonApp (Android native / Linux LE)
sysMonAppQNX (QNX)
sysMon_DSP_Profiler_v2.apk (Android GUI app, Java)
These names can vary slightly by SDK version (6.x), but the folder location is consistent.</p>
<h3 id="put-the-binary-on-your-devicetarget"><a class="header" href="#put-the-binary-on-your-devicetarget">Put the binary on your device/target</a></h3>
<h4 id="android-adb"><a class="header" href="#android-adb">Android (ADB)</a></h4>
<p>Ensure your device is accessible and /system is writable.
Push the binary and make it executable:</p>
<pre><code class="language-bash">adb root
adb remount
adb push $HEXAGON_SDK_ROOT/tools/utils/sysmon/sysMonApp /data/local/tmp/
adb shell chmod 777 /data/local/tmp/sysMonApp
</code></pre>
<p>Heads‑up: To profile from Android (guest VM) into the DSP (PVM), FastRPC daemon must be running on the Android side (e.g., /system/bin/adsprpcd). If that daemon isn’t present, you can run SysMon directly on the PVM (e.g., QNX) instead.</p>
<h4 id="qnx-or-linuxle"><a class="header" href="#qnx-or-linuxle">QNX (or Linux‑LE)</a></h4>
<p>Copy the matching sysMonAppQNX (or sysMonAppLE) onto the target filesystem and run it there.</p>
<h3 id="quickstart-capture-highlevel-dsp-utilization"><a class="header" href="#quickstart-capture-highlevel-dsp-utilization">Quickstart: capture high‑level DSP utilization</a></h3>
<p>Pick your DSP with –q6 (defaults to adsp). Options typically include adsp, cdsp, sdsp, mdsp.</p>
<h4 id="qnx-example"><a class="header" href="#qnx-example">QNX example</a></h4>
<pre><code class="language-bash">sysMonAppQNX profiler --q6 adsp --duration 60
# Outputs (by default): /data/sysmon.bin
</code></pre>
<p><code>--duration &lt;sec&gt;</code> controls capture length (default ~10s).
For MDSP, the file name becomes <code>sysmon_mdsp.bin</code>.</p>
<h4 id="android-native-example"><a class="header" href="#android-native-example">Android native example</a></h4>
<pre><code class="language-bash">shell /data/local/tmp/sysMonApp profiler --q6 cdsp --duration 30
# Then pull the bin:
adb pull /data/sysmon.bin
</code></pre>
<p>If <code>--q6 cdsp</code> <a href="https://mysupport.qualcomm.com/supportforums/s/question/0D5dK000004mN1kSAE/i-installed-hexagon-6002-to-monitor-npu-usage-on-a-mi-14-device-sysmon-works-with-the-adsp-flag-but-fails-with-cdsp-since-ai-app-monitoring-needs-the-cdsp-flag-how-can-i-fix-this">fails on your device</a>, check chip support/firmware and FastRPC daemon availability; there are known device‑specific limitations and you may need OEM images or support to enable CDSP monitoring.</p>
<h3 id="threadlevel-performance-pmu-sampling"><a class="header" href="#threadlevel-performance-pmu-sampling">Thread‑Level Performance (PMU) sampling</a></h3>
<p>To sample PMU counters and per‑thread stats (TLP = thread‑level performance), use the <code>tlp</code> subcommand.</p>
<h4 id="qnx-example-tlp--overall-profile"><a class="header" href="#qnx-example-tlp--overall-profile">QNX example (TLP + overall profile)</a></h4>
<pre><code class="language-bash">sysMonAppQNX tlp \
  --profile 1 \
  --samplingPeriod 1 \
  --q6 adsp \
  --defaultSetEnable 3 \
  --duration 60

# Outputs: /data/sysmontlp_adsp.bin (or sysmontlp_mdsp.bin for MDSP)
</code></pre>
<p>Key flags:</p>
<ul>
<li><code>--samplingPeriod</code> <ms> controls sample period (default ~50 ms).</ms></li>
<li><code>--defaultSetEnable</code> selects logging preset (common values 0–3;
use 3 for rich PMU capture on newer tools).
The public docs describe 0/1/2 modes;
field practice often uses 3 with newer SysMon builds.</li>
</ul>
<p><strong>Important constraint:</strong> only one PMU client can own the hardware at a time.
Don’t run SysMon concurrently with other PMU consumers (e.g., SDP, itrace).
If you see “register failed” errors, stop the other tool
or reboot the system to release PMU.</p>
<h3 id="typical-workflow-android-device--cdspadsp"><a class="header" href="#typical-workflow-android-device--cdspadsp">Typical workflow (Android device → CDSP/ADSP)</a></h3>
<h4 id="prep"><a class="header" href="#prep">Prep</a></h4>
<p>Push sysMonApp to <code>/data/local/tmp</code>.
Confirm the right FastRPC daemon is present for your target DSP
(e.g., adsprpcd for ADSP; device‑specific setup for CDSP).</p>
<h4 id="record"><a class="header" href="#record">Record</a></h4>
<pre><code class="language-bash">adb shell /data/local/tmp/sysMonApp profiler --q6 adsp --duration 20
adb shell /data/local/tmp/sysMonApp tlp --profile 1 --samplingPeriod 2 --q6 adsp --defaultSetEnable 3 --duration 20
</code></pre>
<h4 id="collect"><a class="header" href="#collect">Collect</a></h4>
<pre><code class="language-bash">adb pull /data/sysmon.bin .
adb pull /data/sysmontlp_adsp.bin
</code></pre>
<h4 id="analyze"><a class="header" href="#analyze">Analyze</a></h4>
<p>SysMon’s binary logs can be parsed using sysmon’s parser <code>sysmon_parser</code>.</p>
<pre><code class="language-bash"># Pull the binary log from the device
adb pull /data/sysmon.bin
# Create HTML pages --&gt; sysmon_report_YYYYMMDD_HHMM/sysmon_report.html
sysmon_parser sysmon.bin
</code></pre>
<p>Note that the <code>.bin</code> name varies based on monitoring options.</p>
<h4 id="troubleshooting-tips"><a class="header" href="#troubleshooting-tips">Troubleshooting tips</a></h4>
<p>Some common sysmon-related issues and their solutions are described in
mysupport.qualcomm.com and stack overflow.</p>
<p><strong>Disclaimer</strong>: <code>sysmon</code> is entirely independent from Ripple,
and Ripple support won’t be able to assist you with <code>sysmon</code> issues.</p>
<h2 id="whole-system-profiling-with-qualcomm-r-profiler"><a class="header" href="#whole-system-profiling-with-qualcomm-r-profiler">Whole-system profiling with Qualcomm (R) Profiler</a></h2>
<p>The Qualcomm Profiler is a higher-level tool to monitor the whole system.
Please consult the <a href="https://www.qualcomm.com/developer/software/qualcomm-profiler#profiler">product webpage</a> for more information and support.</p>
<h2 id="profiling-in-qnn-htp"><a class="header" href="#profiling-in-qnn-htp">Profiling in QNN-HTP</a></h2>
<p>Profiling within QNN-HTP is detailed for Qualcomm internal developers
in the <code>docs/profiling.md</code> document in the HexNN repository.</p>
<hr>
<h2 id="hexagon-is-a-registered-trademark-of-qualcomm-incorporated"><a class="header" href="#hexagon-is-a-registered-trademark-of-qualcomm-incorporated">Hexagon is a registered trademark of Qualcomm Incorporated.</a></h2>
<p><em>Copyright (c) 2024-2025 Qualcomm Innovation Center, Inc. All rights reserved.
SPDX-License-Identifier: BSD-3-Clause-Clear</em></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="debugging"><a class="header" href="#debugging">Debugging</a></h1>
<p>Being part of clang, Ripple is compatible with LLVM’s debugger, LLDB.
This section explains how to set up the LLDB plugin on VS Code.</p>
<h2 id="from-microsoft-vs-coder"><a class="header" href="#from-microsoft-vs-coder">From Microsoft VS Code(R)</a></h2>
<p>VS Code(R) is a modern integrated development environment (IDE) published by Microsoft.
It supports LLDB through an extension called “LLDB DAP”.</p>
<p>To install the extension:</p>
<ul>
<li>go to the <code>Extensions</code> menu (typically on the left hand side)</li>
<li>search for <code>LLDB DAP</code></li>
<li>An <code>LLDB DAP</code> extension should appear, published by <code>LLVM</code>. Click on its <code>install</code> button.</li>
<li>Open settings (File-&gt;Preferences-&gt;Settings), in the Workspace tab, and change Lldb-dap: Executable-path to point to the lldb-dap[.exe] you want to use.</li>
</ul>
<p>Set up .vscode/launch.json as appropriate in your project, as for example:</p>
<pre><code class="language-json">{
    // Sample launch.json for Hexagon tools
    "version": "0.2.0",
    "configurations": [
        {
            "name": "(lldb-dap) Launch",
            "type": "lldb-dap",
            "request": "launch",
            "program": "${workspaceFolder}/a.out",
            "args": [],
            "stopOnEntry": false,
            "cwd": "${workspaceFolder}",
            "env": [],
        }
    ]
}
</code></pre>
<p>More examples are available in the Hexagon clang release, <code>Examples/vscode/*/.vscode</code></p>
<p>More info about launch.json is at https://code.visualstudio.com/docs/debugtest/debugging-configuration .</p>
<p>More info about launch.json options for the LLDB DAP extension is in the readme at https://github.com/llvm/vscode-lldb.</p>
<hr>
<p>Visual Studio Code and VS Code are trademarks of Microsoft Corporation.</p>
<hr>
<p><em>Copyright (c) 2024-2025 Qualcomm Innovation Center, Inc. All rights reserved.
SPDX-License-Identifier: BSD-3-Clause-Clear</em></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
